<!DOCTYPE html>
<html>
<head>
<title>Paper collected by Wang</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<style type="text/css">


/* BODY
=============================================================================*/

body {
  font-family: Helvetica, arial, freesans, clean, sans-serif;
  font-size: 14px;
  line-height: 1.6;
  color: #333;
  background-color: #fff;
  padding: 20px;
  max-width: 960px;
  margin: 0 auto;
}

span#pid {
  color:red;
  
}
span#filename{
  font-style: oblique;
}

span#title{
  font-family: Times New Roman, freesans, clean, sans-serif;
  font-style: italic;
  font-size: 20px;
  border:1px solid #B50;
}
span#abs{
  font-family: Times New Roman, freesans, clean, sans-serif;
  font-style: oblique;
  font-size: 18px;
}
</style>
</head>
<body>

</p></br></br><div id='section'>Paperid: <span id='pid'>1, <a href='https://arxiv.org/pdf/2509.00378.pdf' target='_blank'>https://arxiv.org/pdf/2509.00378.pdf</a></span>   <span><a href='https://github.com/shumpei-takezaki/NoiseCutMix' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Shumpei Takezaki, Ryoma Bise, Shinnosuke Matsuo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.00378">NoiseCutMix: A Novel Data Augmentation Approach by Mixing Estimated Noise in Diffusion Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this study, we propose a novel data augmentation method that introduces the concept of CutMix into the generation process of diffusion models, thereby exploiting both the ability of diffusion models to generate natural and high-resolution images and the characteristic of CutMix, which combines features from two classes to create diverse augmented data. Representative data augmentation methods for combining images from multiple classes include CutMix and MixUp. However, techniques like CutMix often result in unnatural boundaries between the two images due to contextual differences. Therefore, in this study, we propose a method, called NoiseCutMix, to achieve natural, high-resolution image generation featuring the fused characteristics of two classes by partially combining the estimated noise corresponding to two different classes in a diffusion model. In the classification experiments, we verified the effectiveness of the proposed method by comparing it with conventional data augmentation techniques that combine multiple classes, random image generation using Stable Diffusion, and combinations of these methods. Our codes are available at: https://github.com/shumpei-takezaki/NoiseCutMix
<div id='section'>Paperid: <span id='pid'>2, <a href='https://arxiv.org/pdf/2503.16322.pdf' target='_blank'>https://arxiv.org/pdf/2503.16322.pdf</a></span>   <span><a href='https://github.com/Huage001/URAE' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/Huage001/URAE' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Ruonan Yu, Songhua Liu, Zhenxiong Tan, Xinchao Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.16322">Ultra-Resolution Adaptation with Ease</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Text-to-image diffusion models have achieved remarkable progress in recent years. However, training models for high-resolution image generation remains challenging, particularly when training data and computational resources are limited. In this paper, we explore this practical problem from two key perspectives: data and parameter efficiency, and propose a set of key guidelines for ultra-resolution adaptation termed \emph{URAE}. For data efficiency, we theoretically and empirically demonstrate that synthetic data generated by some teacher models can significantly promote training convergence. For parameter efficiency, we find that tuning minor components of the weight matrices outperforms widely-used low-rank adapters when synthetic data are unavailable, offering substantial performance gains while maintaining efficiency. Additionally, for models leveraging guidance distillation, such as FLUX, we show that disabling classifier-free guidance, \textit{i.e.}, setting the guidance scale to 1 during adaptation, is crucial for satisfactory performance. Extensive experiments validate that URAE achieves comparable 2K-generation performance to state-of-the-art closed-source models like FLUX1.1 [Pro] Ultra with only 3K samples and 2K iterations, while setting new benchmarks for 4K-resolution generation. Codes are available \href{https://github.com/Huage001/URAE}{here}.
<div id='section'>Paperid: <span id='pid'>3, <a href='https://arxiv.org/pdf/2503.15686.pdf' target='_blank'>https://arxiv.org/pdf/2503.15686.pdf</a></span>   <span><a href='https://github.com/jqliu09/mcld' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiaqi Liu, Jichao Zhang, Paolo Rota, Nicu Sebe
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.15686">Multi-focal Conditioned Latent Diffusion for Person Image Synthesis</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The Latent Diffusion Model (LDM) has demonstrated strong capabilities in high-resolution image generation and has been widely employed for Pose-Guided Person Image Synthesis (PGPIS), yielding promising results. However, the compression process of LDM often results in the deterioration of details, particularly in sensitive areas such as facial features and clothing textures. In this paper, we propose a Multi-focal Conditioned Latent Diffusion (MCLD) method to address these limitations by conditioning the model on disentangled, pose-invariant features from these sensitive regions. Our approach utilizes a multi-focal condition aggregation module, which effectively integrates facial identity and texture-specific information, enhancing the model's ability to produce appearance realistic and identity-consistent images. Our method demonstrates consistent identity and appearance generation on the DeepFashion dataset and enables flexible person image editing due to its generation consistency. The code is available at https://github.com/jqliu09/mcld.
<div id='section'>Paperid: <span id='pid'>4, <a href='https://arxiv.org/pdf/2502.05179.pdf' target='_blank'>https://arxiv.org/pdf/2502.05179.pdf</a></span>   <span><a href='https://github.com/FoundationVision/FlashVideo' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Shilong Zhang, Wenbo Li, Shoufa Chen, Chongjian Ge, Peize Sun, Yida Zhang, Yi Jiang, Zehuan Yuan, Binyue Peng, Ping Luo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.05179">FlashVideo: Flowing Fidelity to Detail for Efficient High-Resolution Video Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>DiT diffusion models have achieved great success in text-to-video generation, leveraging their scalability in model capacity and data scale. High content and motion fidelity aligned with text prompts, however, often require large model parameters and a substantial number of function evaluations (NFEs). Realistic and visually appealing details are typically reflected in high resolution outputs, further amplifying computational demands especially for single stage DiT models. To address these challenges, we propose a novel two stage framework, FlashVideo, which strategically allocates model capacity and NFEs across stages to balance generation fidelity and quality. In the first stage, prompt fidelity is prioritized through a low resolution generation process utilizing large parameters and sufficient NFEs to enhance computational efficiency. The second stage establishes flow matching between low and high resolutions, effectively generating fine details with minimal NFEs. Quantitative and visual results demonstrate that FlashVideo achieves state-of-the-art high resolution video generation with superior computational efficiency. Additionally, the two-stage design enables users to preview the initial output and accordingly adjust the prompt before committing to full-resolution generation, thereby significantly reducing computational costs and wait times as well as enhancing commercial viability.
<div id='section'>Paperid: <span id='pid'>5, <a href='https://arxiv.org/pdf/2411.13552.pdf' target='_blank'>https://arxiv.org/pdf/2411.13552.pdf</a></span>   <span><a href='https://github.com/microsoft/Reducio-VAE' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Rui Tian, Qi Dai, Jianmin Bao, Kai Qiu, Yifan Yang, Chong Luo, Zuxuan Wu, Yu-Gang Jiang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.13552">REDUCIO! Generating 1K Video within 16 Seconds using Extremely Compressed Motion Latents</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Commercial video generation models have exhibited realistic, high-fidelity results but are still restricted to limited access. One crucial obstacle for large-scale applications is the expensive training and inference cost. In this paper, we argue that videos contain significantly more redundant information than images, allowing them to be encoded with very few motion latents. Towards this goal, we design an image-conditioned VAE that projects videos into extremely compressed latent space and decode them based on content images. This magic Reducio charm enables 64x reduction of latents compared to a common 2D VAE, without sacrificing the quality. Building upon Reducio-VAE, we can train diffusion models for high-resolution video generation efficiently. Specifically, we adopt a two-stage generation paradigm, first generating a condition image via text-to-image generation, followed by text-image-to-video generation with the proposed Reducio-DiT. Extensive experiments show that our model achieves strong performance in evaluation. More importantly, our method significantly boosts the training and inference efficiency of video LDMs. Reducio-DiT is trained in just 3.2K A100 GPU hours in total and can generate a 16-frame 1024$\times$1024 video clip within 15.5 seconds on a single A100 GPU. Code released at https://github.com/microsoft/Reducio-VAE .
<div id='section'>Paperid: <span id='pid'>6, <a href='https://arxiv.org/pdf/2410.06055.pdf' target='_blank'>https://arxiv.org/pdf/2410.06055.pdf</a></span>   <span><a href='https://github.com/kmittle/AP-LDM' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Boyuan Cao, Jiaxin Ye, Yujie Wei, Hongming Shan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.06055">AP-LDM: Attentive and Progressive Latent Diffusion Model for Training-Free High-Resolution Image Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Latent diffusion models (LDMs), such as Stable Diffusion, often experience significant structural distortions when directly generating high-resolution (HR) images that exceed their original training resolutions. A straightforward and cost-effective solution is to adapt pre-trained LDMs for HR image generation; however, existing methods often suffer from poor image quality and long inference time. In this paper, we propose an Attentive and Progressive LDM (AP-LDM), a novel, training-free framework aimed at enhancing HR image quality while accelerating the generation process. AP-LDM decomposes the denoising process of LDMs into two stages: (i) attentive training-resolution denoising, and (ii) progressive high-resolution denoising. The first stage generates a latent representation of a higher-quality training-resolution image through the proposed attentive guidance, which utilizes a novel parameter-free self-attention mechanism to enhance the structural consistency. The second stage progressively performs upsampling in pixel space, alleviating the severe artifacts caused by latent space upsampling. Leveraging the effective initialization from the first stage enables denoising at higher resolutions with significantly fewer steps, enhancing overall efficiency. Extensive experimental results demonstrate that AP-LDM significantly outperforms state-of-the-art methods, delivering up to a 5x speedup in HR image generation, thereby highlighting its substantial advantages for real-world applications. Code is available at https://github.com/kmittle/AP-LDM.
<div id='section'>Paperid: <span id='pid'>7, <a href='https://arxiv.org/pdf/2405.14224.pdf' target='_blank'>https://arxiv.org/pdf/2405.14224.pdf</a></span>   <span><a href='https://github.com/tyshiwo1/DiM-DiffusionMamba/' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/tyshiwo1/DiM-DiffusionMamba/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yao Teng, Yue Wu, Han Shi, Xuefei Ning, Guohao Dai, Yu Wang, Zhenguo Li, Xihui Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.14224">DiM: Diffusion Mamba for Efficient High-Resolution Image Synthesis</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Diffusion models have achieved great success in image generation, with the backbone evolving from U-Net to Vision Transformers. However, the computational cost of Transformers is quadratic to the number of tokens, leading to significant challenges when dealing with high-resolution images. In this work, we propose Diffusion Mamba (DiM), which combines the efficiency of Mamba, a sequence model based on State Space Models (SSM), with the expressive power of diffusion models for efficient high-resolution image synthesis. To address the challenge that Mamba cannot generalize to 2D signals, we make several architecture designs including multi-directional scans, learnable padding tokens at the end of each row and column, and lightweight local feature enhancement. Our DiM architecture achieves inference-time efficiency for high-resolution images. In addition, to further improve training efficiency for high-resolution image generation with DiM, we investigate "weak-to-strong" training strategy that pretrains DiM on low-resolution images ($256\times 256$) and then finetune it on high-resolution images ($512 \times 512$). We further explore training-free upsampling strategies to enable the model to generate higher-resolution images (e.g., $1024\times 1024$ and $1536\times 1536$) without further fine-tuning. Experiments demonstrate the effectiveness and efficiency of our DiM. The code of our work is available here: {\url{https://github.com/tyshiwo1/DiM-DiffusionMamba/}}.
<div id='section'>Paperid: <span id='pid'>8, <a href='https://arxiv.org/pdf/2402.03666.pdf' target='_blank'>https://arxiv.org/pdf/2402.03666.pdf</a></span>   <span><a href='https://github.com/hatchetProject/QuEST' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Haoxuan Wang, Yuzhang Shang, Zhihang Yuan, Junyi Wu, Junchi Yan, Yan Yan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.03666">QuEST: Low-bit Diffusion Model Quantization via Efficient Selective Finetuning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The practical deployment of diffusion models is still hindered by the high memory and computational overhead. Although quantization paves a way for model compression and acceleration, existing methods face challenges in achieving low-bit quantization efficiently. In this paper, we identify imbalanced activation distributions as a primary source of quantization difficulty, and propose to adjust these distributions through weight finetuning to be more quantization-friendly. We provide both theoretical and empirical evidence supporting finetuning as a practical and reliable solution. Building on this approach, we further distinguish two critical types of quantized layers: those responsible for retaining essential temporal information and those particularly sensitive to bit-width reduction. By selectively finetuning these layers under both local and global supervision, we mitigate performance degradation while enhancing quantization efficiency. Our method demonstrates its efficacy across three high-resolution image generation tasks, obtaining state-of-the-art performance across multiple bit-width settings.
<div id='section'>Paperid: <span id='pid'>9, <a href='https://arxiv.org/pdf/2310.07702.pdf' target='_blank'>https://arxiv.org/pdf/2310.07702.pdf</a></span>   <span><a href='https://github.com/YingqingHe/ScaleCrafter' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yingqing He, Shaoshu Yang, Haoxin Chen, Xiaodong Cun, Menghan Xia, Yong Zhang, Xintao Wang, Ran He, Qifeng Chen, Ying Shan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2310.07702">ScaleCrafter: Tuning-free Higher-Resolution Visual Generation with Diffusion Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this work, we investigate the capability of generating images from pre-trained diffusion models at much higher resolutions than the training image sizes. In addition, the generated images should have arbitrary image aspect ratios. When generating images directly at a higher resolution, 1024 x 1024, with the pre-trained Stable Diffusion using training images of resolution 512 x 512, we observe persistent problems of object repetition and unreasonable object structures. Existing works for higher-resolution generation, such as attention-based and joint-diffusion approaches, cannot well address these issues. As a new perspective, we examine the structural components of the U-Net in diffusion models and identify the crucial cause as the limited perception field of convolutional kernels. Based on this key observation, we propose a simple yet effective re-dilation that can dynamically adjust the convolutional perception field during inference. We further propose the dispersed convolution and noise-damped classifier-free guidance, which can enable ultra-high-resolution image generation (e.g., 4096 x 4096). Notably, our approach does not require any training or optimization. Extensive experiments demonstrate that our approach can address the repetition issue well and achieve state-of-the-art performance on higher-resolution image synthesis, especially in texture details. Our work also suggests that a pre-trained diffusion model trained on low-resolution images can be directly used for high-resolution visual generation without further tuning, which may provide insights for future research on ultra-high-resolution image and video synthesis.
<div id='section'>Paperid: <span id='pid'>10, <a href='https://arxiv.org/pdf/2408.11001.pdf' target='_blank'>https://arxiv.org/pdf/2408.11001.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Haoning Wu, Shaocheng Shen, Qiang Hu, Xiaoyun Zhang, Ya Zhang, Yanfeng Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.11001">MegaFusion: Extend Diffusion Models towards Higher-resolution Image Generation without Further Tuning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Diffusion models have emerged as frontrunners in text-to-image generation, but their fixed image resolution during training often leads to challenges in high-resolution image generation, such as semantic deviations and object replication. This paper introduces MegaFusion, a novel approach that extends existing diffusion-based text-to-image models towards efficient higher-resolution generation without additional fine-tuning or adaptation. Specifically, we employ an innovative truncate and relay strategy to bridge the denoising processes across different resolutions, allowing for high-resolution image generation in a coarse-to-fine manner. Moreover, by integrating dilated convolutions and noise re-scheduling, we further adapt the model's priors for higher resolution. The versatility and efficacy of MegaFusion make it universally applicable to both latent-space and pixel-space diffusion models, along with other derivative models. Extensive experiments confirm that MegaFusion significantly boosts the capability of existing models to produce images of megapixels and various aspect ratios, while only requiring about 40% of the original computational cost.
<div id='section'>Paperid: <span id='pid'>11, <a href='https://arxiv.org/pdf/2504.06232.pdf' target='_blank'>https://arxiv.org/pdf/2504.06232.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiazi Bu, Pengyang Ling, Yujie Zhou, Pan Zhang, Tong Wu, Xiaoyi Dong, Yuhang Zang, Yuhang Cao, Dahua Lin, Jiaqi Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.06232">HiFlow: Training-free High-Resolution Image Generation with Flow-Aligned Guidance</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Text-to-image (T2I) diffusion/flow models have drawn considerable attention recently due to their remarkable ability to deliver flexible visual creations. Still, high-resolution image synthesis presents formidable challenges due to the scarcity and complexity of high-resolution content. Recent approaches have investigated training-free strategies to enable high-resolution image synthesis with pre-trained models. However, these techniques often struggle with generating high-quality visuals and tend to exhibit artifacts or low-fidelity details, as they typically rely solely on the endpoint of the low-resolution sampling trajectory while neglecting intermediate states that are critical for preserving structure and synthesizing finer detail. To this end, we present HiFlow, a training-free and model-agnostic framework to unlock the resolution potential of pre-trained flow models. Specifically, HiFlow establishes a virtual reference flow within the high-resolution space that effectively captures the characteristics of low-resolution flow information, offering guidance for high-resolution generation through three key aspects: initialization alignment for low-frequency consistency, direction alignment for structure preservation, and acceleration alignment for detail fidelity. By leveraging such flow-aligned guidance, HiFlow substantially elevates the quality of high-resolution image synthesis of T2I models and demonstrates versatility across their personalized variants. Extensive experiments validate HiFlow's capability in achieving superior high-resolution image quality over state-of-the-art methods.
<div id='section'>Paperid: <span id='pid'>12, <a href='https://arxiv.org/pdf/2406.07792.pdf' target='_blank'>https://arxiv.org/pdf/2406.07792.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ivan Skorokhodov, Willi Menapace, Aliaksandr Siarohin, Sergey Tulyakov
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.07792">Hierarchical Patch Diffusion Models for High-Resolution Video Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Diffusion models have demonstrated remarkable performance in image and video synthesis. However, scaling them to high-resolution inputs is challenging and requires restructuring the diffusion pipeline into multiple independent components, limiting scalability and complicating downstream applications. This makes it very efficient during training and unlocks end-to-end optimization on high-resolution videos. We improve PDMs in two principled ways. First, to enforce consistency between patches, we develop deep context fusion -- an architectural technique that propagates the context information from low-scale to high-scale patches in a hierarchical manner. Second, to accelerate training and inference, we propose adaptive computation, which allocates more network capacity and computation towards coarse image details. The resulting model sets a new state-of-the-art FVD score of 66.32 and Inception Score of 87.68 in class-conditional video generation on UCF-101 $256^2$, surpassing recent methods by more than 100%. Then, we show that it can be rapidly fine-tuned from a base $36\times 64$ low-resolution generator for high-resolution $64 \times 288 \times 512$ text-to-video synthesis. To the best of our knowledge, our model is the first diffusion-based architecture which is trained on such high resolutions entirely end-to-end. Project webpage: https://snap-research.github.io/hpdm.
<div id='section'>Paperid: <span id='pid'>13, <a href='https://arxiv.org/pdf/2311.11325.pdf' target='_blank'>https://arxiv.org/pdf/2311.11325.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jingyun Liang, Yuchen Fan, Kai Zhang, Radu Timofte, Luc Van Gool, Rakesh Ranjan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2311.11325">MoVideo: Motion-Aware Video Generation with Diffusion Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>While recent years have witnessed great progress on using diffusion models for video generation, most of them are simple extensions of image generation frameworks, which fail to explicitly consider one of the key differences between videos and images, i.e., motion. In this paper, we propose a novel motion-aware video generation (MoVideo) framework that takes motion into consideration from two aspects: video depth and optical flow. The former regulates motion by per-frame object distances and spatial layouts, while the later describes motion by cross-frame correspondences that help in preserving fine details and improving temporal consistency. More specifically, given a key frame that exists or generated from text prompts, we first design a diffusion model with spatio-temporal modules to generate the video depth and the corresponding optical flows. Then, the video is generated in the latent space by another spatio-temporal diffusion model under the guidance of depth, optical flow-based warped latent video and the calculated occlusion mask. Lastly, we use optical flows again to align and refine different frames for better video decoding from the latent space to the pixel space. In experiments, MoVideo achieves state-of-the-art results in both text-to-video and image-to-video generation, showing promising prompt consistency, frame consistency and visual quality.
<div id='section'>Paperid: <span id='pid'>14, <a href='https://arxiv.org/pdf/2502.04116.pdf' target='_blank'>https://arxiv.org/pdf/2502.04116.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Junhao Song, Yichao Zhang, Ziqian Bi, Tianyang Wang, Keyu Chen, Ming Li, Qian Niu, Junyu Liu, Benji Peng, Sen Zhang, Ming Liu, Jiawei Xu, Xuanhe Pan, Jinlang Wang, Pohsun Feng, Yizhu Wen, Lawrence K. Q. Yan, Hong-Ming Tseng, Xinyuan Song, Jintao Ren, Silin Chen, Yunze Wang, Weiche Hsieh, Bowen Jing, Junjie Yang, Jun Zhou, Zheyu Yao, Chia Xin Liang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.04116">Generative Adversarial Networks Bridging Art and Machine Intelligence</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Generative Adversarial Networks (GAN) have greatly influenced the development of computer vision and artificial intelligence in the past decade and also connected art and machine intelligence together. This book begins with a detailed introduction to the fundamental principles and historical development of GANs, contrasting them with traditional generative models and elucidating the core adversarial mechanisms through illustrative Python examples. The text systematically addresses the mathematical and theoretical underpinnings including probability theory, statistics, and game theory providing a solid framework for understanding the objectives, loss functions, and optimisation challenges inherent to GAN training. Subsequent chapters review classic variants such as Conditional GANs, DCGANs, InfoGAN, and LAPGAN before progressing to advanced training methodologies like Wasserstein GANs, GANs with gradient penalty, least squares GANs, and spectral normalisation techniques. The book further examines architectural enhancements and task-specific adaptations in generators and discriminators, showcasing practical implementations in high resolution image generation, artistic style transfer, video synthesis, text to image generation and other multimedia applications. The concluding sections offer insights into emerging research trends, including self-attention mechanisms, transformer-based generative models, and a comparative analysis with diffusion models, thus charting promising directions for future developments in both academic and applied settings.
<div id='section'>Paperid: <span id='pid'>15, <a href='https://arxiv.org/pdf/2308.13812.pdf' target='_blank'>https://arxiv.org/pdf/2308.13812.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hao Fei, Shengqiong Wu, Wei Ji, Hanwang Zhang, Tat-Seng Chua
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2308.13812">Dysen-VDM: Empowering Dynamics-aware Text-to-Video Diffusion with LLMs</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Text-to-video (T2V) synthesis has gained increasing attention in the community, in which the recently emerged diffusion models (DMs) have promisingly shown stronger performance than the past approaches. While existing state-of-the-art DMs are competent to achieve high-resolution video generation, they may largely suffer from key limitations (e.g., action occurrence disorders, crude video motions) with respect to the intricate temporal dynamics modeling, one of the crux of video synthesis. In this work, we investigate strengthening the awareness of video dynamics for DMs, for high-quality T2V generation. Inspired by human intuition, we design an innovative dynamic scene manager (dubbed as Dysen) module, which includes (step-1) extracting from input text the key actions with proper time-order arrangement, (step-2) transforming the action schedules into the dynamic scene graph (DSG) representations, and (step-3) enriching the scenes in the DSG with sufficient and reasonable details. Taking advantage of the existing powerful LLMs (e.g., ChatGPT) via in-context learning, Dysen realizes (nearly) human-level temporal dynamics understanding. Finally, the resulting video DSG with rich action scene details is encoded as fine-grained spatio-temporal features, integrated into the backbone T2V DM for video generating. Experiments on popular T2V datasets suggest that our Dysen-VDM consistently outperforms prior arts with significant margins, especially in scenarios with complex actions. Codes at https://haofei.vip/Dysen-VDM
<div id='section'>Paperid: <span id='pid'>16, <a href='https://arxiv.org/pdf/2403.04692.pdf' target='_blank'>https://arxiv.org/pdf/2403.04692.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Junsong Chen, Chongjian Ge, Enze Xie, Yue Wu, Lewei Yao, Xiaozhe Ren, Zhongdao Wang, Ping Luo, Huchuan Lu, Zhenguo Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.04692">PixArt-Î£: Weak-to-Strong Training of Diffusion Transformer for 4K Text-to-Image Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper, we introduce PixArt-Î£, a Diffusion Transformer model~(DiT) capable of directly generating images at 4K resolution. PixArt-Î£represents a significant advancement over its predecessor, PixArt-Î±, offering images of markedly higher fidelity and improved alignment with text prompts. A key feature of PixArt-Î£is its training efficiency. Leveraging the foundational pre-training of PixArt-Î±, it evolves from the `weaker' baseline to a `stronger' model via incorporating higher quality data, a process we term "weak-to-strong training". The advancements in PixArt-Î£are twofold: (1) High-Quality Training Data: PixArt-Î£incorporates superior-quality image data, paired with more precise and detailed image captions. (2) Efficient Token Compression: we propose a novel attention module within the DiT framework that compresses both keys and values, significantly improving efficiency and facilitating ultra-high-resolution image generation. Thanks to these improvements, PixArt-Î£achieves superior image quality and user prompt adherence capabilities with significantly smaller model size (0.6B parameters) than existing text-to-image diffusion models, such as SDXL (2.6B parameters) and SD Cascade (5.1B parameters). Moreover, PixArt-Î£'s capability to generate 4K images supports the creation of high-resolution posters and wallpapers, efficiently bolstering the production of high-quality visual content in industries such as film and gaming.
<div id='section'>Paperid: <span id='pid'>17, <a href='https://arxiv.org/pdf/2504.14470.pdf' target='_blank'>https://arxiv.org/pdf/2504.14470.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jingjing Ren, Wenbo Li, Zhongdao Wang, Haoze Sun, Bangzhen Liu, Haoyu Chen, Jiaqi Xu, Aoxue Li, Shifeng Zhang, Bin Shao, Yong Guo, Lei Zhu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.14470">Turbo2K: Towards Ultra-Efficient and High-Quality 2K Video Synthesis</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Demand for 2K video synthesis is rising with increasing consumer expectations for ultra-clear visuals. While diffusion transformers (DiTs) have demonstrated remarkable capabilities in high-quality video generation, scaling them to 2K resolution remains computationally prohibitive due to quadratic growth in memory and processing costs. In this work, we propose Turbo2K, an efficient and practical framework for generating detail-rich 2K videos while significantly improving training and inference efficiency. First, Turbo2K operates in a highly compressed latent space, reducing computational complexity and memory footprint, making high-resolution video synthesis feasible. However, the high compression ratio of the VAE and limited model size impose constraints on generative quality. To mitigate this, we introduce a knowledge distillation strategy that enables a smaller student model to inherit the generative capacity of a larger, more powerful teacher model. Our analysis reveals that, despite differences in latent spaces and architectures, DiTs exhibit structural similarities in their internal representations, facilitating effective knowledge transfer. Second, we design a hierarchical two-stage synthesis framework that first generates multi-level feature at lower resolutions before guiding high-resolution video generation. This approach ensures structural coherence and fine-grained detail refinement while eliminating redundant encoding-decoding overhead, further enhancing computational efficiency.Turbo2K achieves state-of-the-art efficiency, generating 5-second, 24fps, 2K videos with significantly reduced computational cost. Compared to existing methods, Turbo2K is up to 20$\times$ faster for inference, making high-resolution video generation more scalable and practical for real-world applications.
<div id='section'>Paperid: <span id='pid'>18, <a href='https://arxiv.org/pdf/2407.02158.pdf' target='_blank'>https://arxiv.org/pdf/2407.02158.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jingjing Ren, Wenbo Li, Haoyu Chen, Renjing Pei, Bin Shao, Yong Guo, Long Peng, Fenglong Song, Lei Zhu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.02158">UltraPixel: Advancing Ultra-High-Resolution Image Synthesis to New Peaks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Ultra-high-resolution image generation poses great challenges, such as increased semantic planning complexity and detail synthesis difficulties, alongside substantial training resource demands. We present UltraPixel, a novel architecture utilizing cascade diffusion models to generate high-quality images at multiple resolutions (\textit{e.g.}, 1K to 6K) within a single model, while maintaining computational efficiency. UltraPixel leverages semantics-rich representations of lower-resolution images in the later denoising stage to guide the whole generation of highly detailed high-resolution images, significantly reducing complexity. Furthermore, we introduce implicit neural representations for continuous upsampling and scale-aware normalization layers adaptable to various resolutions. Notably, both low- and high-resolution processes are performed in the most compact space, sharing the majority of parameters with less than 3$\%$ additional parameters for high-resolution outputs, largely enhancing training and inference efficiency. Our model achieves fast training with reduced data requirements, producing photo-realistic high-resolution images and demonstrating state-of-the-art performance in extensive experiments.
<div id='section'>Paperid: <span id='pid'>19, <a href='https://arxiv.org/pdf/2504.17789.pdf' target='_blank'>https://arxiv.org/pdf/2504.17789.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xu Ma, Peize Sun, Haoyu Ma, Hao Tang, Chih-Yao Ma, Jialiang Wang, Kunpeng Li, Xiaoliang Dai, Yujun Shi, Xuan Ju, Yushi Hu, Artsiom Sanakoyeu, Felix Juefei-Xu, Ji Hou, Junjiao Tian, Tao Xu, Tingbo Hou, Yen-Cheng Liu, Zecheng He, Zijian He, Matt Feiszli, Peizhao Zhang, Peter Vajda, Sam Tsai, Yun Fu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.17789">Token-Shuffle: Towards High-Resolution Image Generation with Autoregressive Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Autoregressive (AR) models, long dominant in language generation, are increasingly applied to image synthesis but are often considered less competitive than Diffusion-based models. A primary limitation is the substantial number of image tokens required for AR models, which constrains both training and inference efficiency, as well as image resolution. To address this, we present Token-Shuffle, a novel yet simple method that reduces the number of image tokens in Transformer. Our key insight is the dimensional redundancy of visual vocabularies in Multimodal Large Language Models (MLLMs), where low-dimensional visual codes from visual encoder are directly mapped to high-dimensional language vocabularies. Leveraging this, we consider two key operations: token-shuffle, which merges spatially local tokens along channel dimension to decrease the input token number, and token-unshuffle, which untangles the inferred tokens after Transformer blocks to restore the spatial arrangement for output. Jointly training with textual prompts, our strategy requires no additional pretrained text-encoder and enables MLLMs to support extremely high-resolution image synthesis in a unified next-token prediction way while maintaining efficient training and inference. For the first time, we push the boundary of AR text-to-image generation to a resolution of 2048x2048 with gratifying generation performance. In GenAI-benchmark, our 2.7B model achieves 0.77 overall score on hard prompts, outperforming AR models LlamaGen by 0.18 and diffusion models LDM by 0.15. Exhaustive large-scale human evaluations also demonstrate our prominent image generation ability in terms of text-alignment, visual flaw, and visual appearance. We hope that Token-Shuffle can serve as a foundational design for efficient high-resolution image generation within MLLMs.
<div id='section'>Paperid: <span id='pid'>20, <a href='https://arxiv.org/pdf/2404.04478.pdf' target='_blank'>https://arxiv.org/pdf/2404.04478.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhengcong Fei, Mingyuan Fan, Changqian Yu, Debang Li, Junshi Huang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.04478">Diffusion-RWKV: Scaling RWKV-Like Architectures for Diffusion Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Transformers have catalyzed advancements in computer vision and natural language processing (NLP) fields. However, substantial computational complexity poses limitations for their application in long-context tasks, such as high-resolution image generation. This paper introduces a series of architectures adapted from the RWKV model used in the NLP, with requisite modifications tailored for diffusion model applied to image generation tasks, referred to as Diffusion-RWKV. Similar to the diffusion with Transformers, our model is designed to efficiently handle patchnified inputs in a sequence with extra conditions, while also scaling up effectively, accommodating both large-scale parameters and extensive datasets. Its distinctive advantage manifests in its reduced spatial aggregation complexity, rendering it exceptionally adept at processing high-resolution images, thereby eliminating the necessity for windowing or group cached operations. Experimental results on both condition and unconditional image generation tasks demonstrate that Diffison-RWKV achieves performance on par with or surpasses existing CNN or Transformer-based diffusion models in FID and IS metrics while significantly reducing total computation FLOP usage.
<div id='section'>Paperid: <span id='pid'>21, <a href='https://arxiv.org/pdf/2403.01505.pdf' target='_blank'>https://arxiv.org/pdf/2403.01505.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hongjian Liu, Qingsong Xie, TianXiang Ye, Zhijie Deng, Chen Chen, Shixiang Tang, Xueyang Fu, Haonan Lu, Zheng-jun Zha
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.01505">SCott: Accelerating Diffusion Models with Stochastic Consistency Distillation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The iterative sampling procedure employed by diffusion models (DMs) often leads to significant inference latency. To address this, we propose Stochastic Consistency Distillation (SCott) to enable accelerated text-to-image generation, where high-quality and diverse generations can be achieved within just 2-4 sampling steps. In contrast to vanilla consistency distillation (CD) which distills the ordinary differential equation solvers-based sampling process of a pre-trained teacher model into a student, SCott explores the possibility and validates the efficacy of integrating stochastic differential equation (SDE) solvers into CD to fully unleash the potential of the teacher. SCott is augmented with elaborate strategies to control the noise strength and sampling process of the SDE solver. An adversarial loss is further incorporated to strengthen the consistency constraints in rare sampling steps. Empirically, on the MSCOCO-2017 5K dataset with a Stable Diffusion-V1.5 teacher, SCott achieves an FID of 21.9 with 2 sampling steps, surpassing that of the 1-step InstaFlow (23.4) and the 4-step UFOGen (22.1). Moreover, SCott can yield more diverse samples than other consistency models for high-resolution image generation, with up to 16% improvement in a qualified metric.
<div id='section'>Paperid: <span id='pid'>22, <a href='https://arxiv.org/pdf/2310.02714.pdf' target='_blank'>https://arxiv.org/pdf/2310.02714.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xuanmeng Zhang, Jianfeng Zhang, Rohan Chacko, Hongyi Xu, Guoxian Song, Yi Yang, Jiashi Feng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2310.02714">GETAvatar: Generative Textured Meshes for Animatable Human Avatars</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We study the problem of 3D-aware full-body human generation, aiming at creating animatable human avatars with high-quality textures and geometries. Generally, two challenges remain in this field: i) existing methods struggle to generate geometries with rich realistic details such as the wrinkles of garments; ii) they typically utilize volumetric radiance fields and neural renderers in the synthesis process, making high-resolution rendering non-trivial. To overcome these problems, we propose GETAvatar, a Generative model that directly generates Explicit Textured 3D meshes for animatable human Avatar, with photo-realistic appearance and fine geometric details. Specifically, we first design an articulated 3D human representation with explicit surface modeling, and enrich the generated humans with realistic surface details by learning from the 2D normal maps of 3D scan data. Second, with the explicit mesh representation, we can use a rasterization-based renderer to perform surface rendering, allowing us to achieve high-resolution image generation efficiently. Extensive experiments demonstrate that GETAvatar achieves state-of-the-art performance on 3D-aware human generation both in appearance and geometry quality. Notably, GETAvatar can generate images at 512x512 resolution with 17FPS and 1024x1024 resolution with 14FPS, improving upon previous methods by 2x. Our code and models will be available.
<div id='section'>Paperid: <span id='pid'>23, <a href='https://arxiv.org/pdf/2304.08818.pdf' target='_blank'>https://arxiv.org/pdf/2304.08818.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Andreas Blattmann, Robin Rombach, Huan Ling, Tim Dockhorn, Seung Wook Kim, Sanja Fidler, Karsten Kreis
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2304.08818">Align your Latents: High-Resolution Video Synthesis with Latent Diffusion Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Latent Diffusion Models (LDMs) enable high-quality image synthesis while avoiding excessive compute demands by training a diffusion model in a compressed lower-dimensional latent space. Here, we apply the LDM paradigm to high-resolution video generation, a particularly resource-intensive task. We first pre-train an LDM on images only; then, we turn the image generator into a video generator by introducing a temporal dimension to the latent space diffusion model and fine-tuning on encoded image sequences, i.e., videos. Similarly, we temporally align diffusion model upsamplers, turning them into temporally consistent video super resolution models. We focus on two relevant real-world applications: Simulation of in-the-wild driving data and creative content creation with text-to-video modeling. In particular, we validate our Video LDM on real driving videos of resolution 512 x 1024, achieving state-of-the-art performance. Furthermore, our approach can easily leverage off-the-shelf pre-trained image LDMs, as we only need to train a temporal alignment model in that case. Doing so, we turn the publicly available, state-of-the-art text-to-image LDM Stable Diffusion into an efficient and expressive text-to-video model with resolution up to 1280 x 2048. We show that the temporal layers trained in this way generalize to different fine-tuned text-to-image LDMs. Utilizing this property, we show the first results for personalized text-to-video generation, opening exciting directions for future content creation. Project page: https://research.nvidia.com/labs/toronto-ai/VideoLDM/
<div id='section'>Paperid: <span id='pid'>24, <a href='https://arxiv.org/pdf/2210.03142.pdf' target='_blank'>https://arxiv.org/pdf/2210.03142.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chenlin Meng, Robin Rombach, Ruiqi Gao, Diederik P. Kingma, Stefano Ermon, Jonathan Ho, Tim Salimans
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2210.03142">On Distillation of Guided Diffusion Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Classifier-free guided diffusion models have recently been shown to be highly effective at high-resolution image generation, and they have been widely used in large-scale diffusion frameworks including DALLE-2, Stable Diffusion and Imagen. However, a downside of classifier-free guided diffusion models is that they are computationally expensive at inference time since they require evaluating two diffusion models, a class-conditional model and an unconditional model, tens to hundreds of times. To deal with this limitation, we propose an approach to distilling classifier-free guided diffusion models into models that are fast to sample from: Given a pre-trained classifier-free guided model, we first learn a single model to match the output of the combined conditional and unconditional models, and then we progressively distill that model to a diffusion model that requires much fewer sampling steps. For standard diffusion models trained on the pixel-space, our approach is able to generate images visually comparable to that of the original model using as few as 4 sampling steps on ImageNet 64x64 and CIFAR-10, achieving FID/IS scores comparable to that of the original model while being up to 256 times faster to sample from. For diffusion models trained on the latent-space (e.g., Stable Diffusion), our approach is able to generate high-fidelity images using as few as 1 to 4 denoising steps, accelerating inference by at least 10-fold compared to existing methods on ImageNet 256x256 and LAION datasets. We further demonstrate the effectiveness of our approach on text-guided image editing and inpainting, where our distilled model is able to generate high-quality results using as few as 2-4 denoising steps.
<div id='section'>Paperid: <span id='pid'>25, <a href='https://arxiv.org/pdf/2411.18552.pdf' target='_blank'>https://arxiv.org/pdf/2411.18552.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Haosen Yang, Adrian Bulat, Isma Hadji, Hai X. Pham, Xiatian Zhu, Georgios Tzimiropoulos, Brais Martinez
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.18552">FAM Diffusion: Frequency and Attention Modulation for High-Resolution Image Generation with Stable Diffusion</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Diffusion models are proficient at generating high-quality images. They are however effective only when operating at the resolution used during training. Inference at a scaled resolution leads to repetitive patterns and structural distortions. Retraining at higher resolutions quickly becomes prohibitive. Thus, methods enabling pre-existing diffusion models to operate at flexible test-time resolutions are highly desirable. Previous works suffer from frequent artifacts and often introduce large latency overheads. We propose two simple modules that combine to solve these issues. We introduce a Frequency Modulation (FM) module that leverages the Fourier domain to improve the global structure consistency, and an Attention Modulation (AM) module which improves the consistency of local texture patterns, a problem largely ignored in prior works. Our method, coined Fam diffusion, can seamlessly integrate into any latent diffusion model and requires no additional training. Extensive qualitative results highlight the effectiveness of our method in addressing structural and local artifacts, while quantitative results show state-of-the-art performance. Also, our method avoids redundant inference tricks for improved consistency such as patch-based or progressive generation, leading to negligible latency overheads.
<div id='section'>Paperid: <span id='pid'>26, <a href='https://arxiv.org/pdf/2508.17756.pdf' target='_blank'>https://arxiv.org/pdf/2508.17756.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Fanjiang Ye, Zepeng Zhao, Yi Mu, Jucheng Shen, Renjie Li, Kaijian Wang, Desen Sun, Saurabh Agarwal, Myungjin Lee, Triston Cao, Aditya Akella, Arvind Krishnamurthy, T. S. Eugene Ng, Zhengzhong Tu, Yuke Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.17756">SuperGen: An Efficient Ultra-high-resolution Video Generation System with Sketching and Tiling</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Diffusion models have recently achieved remarkable success in generative tasks (e.g., image and video generation), and the demand for high-quality content (e.g., 2K/4K videos) is rapidly increasing across various domains. However, generating ultra-high-resolution videos on existing standard-resolution (e.g., 720p) platforms remains challenging due to the excessive re-training requirements and prohibitively high computational and memory costs. To this end, we introduce SuperGen, an efficient tile-based framework for ultra-high-resolution video generation. SuperGen features a novel training-free algorithmic innovation with tiling to successfully support a wide range of resolutions without additional training efforts while significantly reducing both memory footprint and computational complexity. Moreover, SuperGen incorporates a tile-tailored, adaptive, region-aware caching strategy that accelerates video generation by exploiting redundancy across denoising steps and spatial regions. SuperGen also integrates cache-guided, communication-minimized tile parallelism for enhanced throughput and minimized latency. Evaluations demonstrate that SuperGen harvests the maximum performance gains while achieving high output quality across various benchmarks.
<div id='section'>Paperid: <span id='pid'>27, <a href='https://arxiv.org/pdf/2412.05796.pdf' target='_blank'>https://arxiv.org/pdf/2412.05796.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kaiwen Zha, Lijun Yu, Alireza Fathi, David A. Ross, Cordelia Schmid, Dina Katabi, Xiuye Gu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.05796">Language-Guided Image Tokenization for Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Image tokenization, the process of transforming raw image pixels into a compact low-dimensional latent representation, has proven crucial for scalable and efficient image generation. However, mainstream image tokenization methods generally have limited compression rates, making high-resolution image generation computationally expensive. To address this challenge, we propose to leverage language for efficient image tokenization, and we call our method Text-Conditioned Image Tokenization (TexTok). TexTok is a simple yet effective tokenization framework that leverages language to provide a compact, high-level semantic representation. By conditioning the tokenization process on descriptive text captions, TexTok simplifies semantic learning, allowing more learning capacity and token space to be allocated to capture fine-grained visual details, leading to enhanced reconstruction quality and higher compression rates. Compared to the conventional tokenizer without text conditioning, TexTok achieves average reconstruction FID improvements of 29.2% and 48.1% on ImageNet-256 and -512 benchmarks respectively, across varying numbers of tokens. These tokenization improvements consistently translate to 16.3% and 34.3% average improvements in generation FID. By simply replacing the tokenizer in Diffusion Transformer (DiT) with TexTok, our system can achieve a 93.5x inference speedup while still outperforming the original DiT using only 32 tokens on ImageNet-512. TexTok with a vanilla DiT generator achieves state-of-the-art FID scores of 1.46 and 1.62 on ImageNet-256 and -512 respectively. Furthermore, we demonstrate TexTok's superiority on the text-to-image generation task, effectively utilizing the off-the-shelf text captions in tokenization. Project page is at: https://kaiwenzha.github.io/textok/.
<div id='section'>Paperid: <span id='pid'>28, <a href='https://arxiv.org/pdf/2412.03837.pdf' target='_blank'>https://arxiv.org/pdf/2412.03837.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Abul Ehtesham, Saket Kumar, Aditi Singh, Tala Talaei Khoei
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.03837">Movie Gen: SWOT Analysis of Meta's Generative AI Foundation Model for Transforming Media Generation, Advertising, and Entertainment Industries</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Generative AI is reshaping the media landscape, enabling unprecedented capabilities in video creation, personalization, and scalability. This paper presents a comprehensive SWOT analysis of Metas Movie Gen, a cutting-edge generative AI foundation model designed to produce 1080p HD videos with synchronized audio from simple text prompts. We explore its strengths, including high-resolution video generation, precise editing, and seamless audio integration, which make it a transformative tool across industries such as filmmaking, advertising, and education. However, the analysis also addresses limitations, such as constraints on video length and potential biases in generated content, which pose challenges for broader adoption. In addition, we examine the evolving regulatory and ethical considerations surrounding generative AI, focusing on issues like content authenticity, cultural representation, and responsible use. Through comparative insights with leading models like DALL-E and Google Imagen, this paper highlights Movie Gens unique features, such as video personalization and multimodal synthesis, while identifying opportunities for innovation and areas requiring further research. Our findings provide actionable insights for stakeholders, emphasizing both the opportunities and challenges of deploying generative AI in media production. This work aims to guide future advancements in generative AI, ensuring scalability, quality, and ethical integrity in this rapidly evolving field.
<div id='section'>Paperid: <span id='pid'>29, <a href='https://arxiv.org/pdf/2406.07435.pdf' target='_blank'>https://arxiv.org/pdf/2406.07435.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shashank Agnihotri, Julia Grabinski, Janis Keuper, Margret Keuper
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.07435">Beware of Aliases -- Signal Preservation is Crucial for Robust Image Restoration</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Image restoration networks are usually comprised of an encoder and a decoder, responsible for aggregating image content from noisy, distorted data and to restore clean, undistorted images, respectively. Data aggregation as well as high-resolution image generation both usually come at the risk of involving aliases, i.e.~standard architectures put their ability to reconstruct the model input in jeopardy to reach high PSNR values on validation data. The price to be paid is low model robustness. In this work, we show that simply providing alias-free paths in state-of-the-art reconstruction transformers supports improved model robustness at low costs on the restoration performance. We do so by proposing BOA-Restormer, a transformer-based image restoration model that executes downsampling and upsampling operations partly in the frequency domain to ensure alias-free paths along the entire model while potentially preserving all relevant high-frequency information.
<div id='section'>Paperid: <span id='pid'>30, <a href='https://arxiv.org/pdf/2304.04820.pdf' target='_blank'>https://arxiv.org/pdf/2304.04820.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ze Wang, Jiang Wang, Zicheng Liu, Qiang Qiu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2304.04820">Binary Latent Diffusion</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper, we show that a binary latent space can be explored for compact yet expressive image representations. We model the bi-directional mappings between an image and the corresponding latent binary representation by training an auto-encoder with a Bernoulli encoding distribution. On the one hand, the binary latent space provides a compact discrete image representation of which the distribution can be modeled more efficiently than pixels or continuous latent representations. On the other hand, we now represent each image patch as a binary vector instead of an index of a learned cookbook as in discrete image representations with vector quantization. In this way, we obtain binary latent representations that allow for better image quality and high-resolution image representations without any multi-stage hierarchy in the latent space. In this binary latent space, images can now be generated effectively using a binary latent diffusion model tailored specifically for modeling the prior over the binary image representations. We present both conditional and unconditional image generation experiments with multiple datasets, and show that the proposed method performs comparably to state-of-the-art methods while dramatically improving the sampling efficiency to as few as 16 steps without using any test-time acceleration. The proposed framework can also be seamlessly scaled to $1024 \times 1024$ high-resolution image generation without resorting to latent hierarchy or multi-stage refinements.
<div id='section'>Paperid: <span id='pid'>31, <a href='https://arxiv.org/pdf/2310.05400.pdf' target='_blank'>https://arxiv.org/pdf/2310.05400.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shiyue Cao, Yueqin Yin, Lianghua Huang, Yu Liu, Xin Zhao, Deli Zhao, Kaiqi Huang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2310.05400">Efficient-VQGAN: Towards High-Resolution Image Generation with Efficient Vision Transformers</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vector-quantized image modeling has shown great potential in synthesizing high-quality images. However, generating high-resolution images remains a challenging task due to the quadratic computational overhead of the self-attention process. In this study, we seek to explore a more efficient two-stage framework for high-resolution image generation with improvements in the following three aspects. (1) Based on the observation that the first quantization stage has solid local property, we employ a local attention-based quantization model instead of the global attention mechanism used in previous methods, leading to better efficiency and reconstruction quality. (2) We emphasize the importance of multi-grained feature interaction during image generation and introduce an efficient attention mechanism that combines global attention (long-range semantic consistency within the whole image) and local attention (fined-grained details). This approach results in faster generation speed, higher generation fidelity, and improved resolution. (3) We propose a new generation pipeline incorporating autoencoding training and autoregressive generation strategy, demonstrating a better paradigm for image synthesis. Extensive experiments demonstrate the superiority of our approach in high-quality and high-resolution image reconstruction and generation.
<div id='section'>Paperid: <span id='pid'>32, <a href='https://arxiv.org/pdf/2507.12698.pdf' target='_blank'>https://arxiv.org/pdf/2507.12698.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zahra TehraniNasab, Hujun Ni, Amar Kumar, Tal Arbel
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.12698">Pixel Perfect MegaMed: A Megapixel-Scale Vision-Language Foundation Model for Generating High Resolution Medical Images</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Medical image synthesis presents unique challenges due to the inherent complexity and high-resolution details required in clinical contexts. Traditional generative architectures such as Generative Adversarial Networks (GANs) or Variational Auto Encoder (VAEs) have shown great promise for high-resolution image generation but struggle with preserving fine-grained details that are key for accurate diagnosis. To address this issue, we introduce Pixel Perfect MegaMed, the first vision-language foundation model to synthesize images at resolutions of 1024x1024. Our method deploys a multi-scale transformer architecture designed specifically for ultra-high resolution medical image generation, enabling the preservation of both global anatomical context and local image-level details. By leveraging vision-language alignment techniques tailored to medical terminology and imaging modalities, Pixel Perfect MegaMed bridges the gap between textual descriptions and visual representations at unprecedented resolution levels. We apply our model to the CheXpert dataset and demonstrate its ability to generate clinically faithful chest X-rays from text prompts. Beyond visual quality, these high-resolution synthetic images prove valuable for downstream tasks such as classification, showing measurable performance gains when used for data augmentation, particularly in low-data regimes. Our code is accessible through the project website - https://tehraninasab.github.io/pixelperfect-megamed.
<div id='section'>Paperid: <span id='pid'>33, <a href='https://arxiv.org/pdf/2506.18999.pdf' target='_blank'>https://arxiv.org/pdf/2506.18999.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuan Yao, Yicong Hong, Difan Liu, Long Mai, Feng Liu, Jiebo Luo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.18999">Diffusion Transformer-to-Mamba Distillation for High-Resolution Image Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The quadratic computational complexity of self-attention in diffusion transformers (DiT) introduces substantial computational costs in high-resolution image generation. While the linear-complexity Mamba model emerges as a potential alternative, direct Mamba training remains empirically challenging. To address this issue, this paper introduces diffusion transformer-to-mamba distillation (T2MD), forming an efficient training pipeline that facilitates the transition from the self-attention-based transformer to the linear complexity state-space model Mamba. We establish a diffusion self-attention and Mamba hybrid model that simultaneously achieves efficiency and global dependencies. With the proposed layer-level teacher forcing and feature-based knowledge distillation, T2MD alleviates the training difficulty and high cost of a state space model from scratch. Starting from the distilled 512$\times$512 resolution base model, we push the generation towards 2048$\times$2048 images via lightweight adaptation and high-resolution fine-tuning. Experiments demonstrate that our training path leads to low overhead but high-quality text-to-image generation. Importantly, our results also justify the feasibility of using sequential and causal Mamba models for generating non-causal visual output, suggesting the potential for future exploration.
<div id='section'>Paperid: <span id='pid'>34, <a href='https://arxiv.org/pdf/2506.00433.pdf' target='_blank'>https://arxiv.org/pdf/2506.00433.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Luigi Sigillo, Shengfeng He, Danilo Comminiello
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.00433">Latent Wavelet Diffusion For Ultra-High-Resolution Image Synthesis</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>High-resolution image synthesis remains a core challenge in generative modeling, particularly in balancing computational efficiency with the preservation of fine-grained visual detail. We present Latent Wavelet Diffusion (LWD), a lightweight training framework that significantly improves detail and texture fidelity in ultra-high-resolution (2K-4K) image synthesis. LWD introduces a novel, frequency-aware masking strategy derived from wavelet energy maps, which dynamically focuses the training process on detail-rich regions of the latent space. This is complemented by a scale-consistent VAE objective to ensure high spectral fidelity. The primary advantage of our approach is its efficiency: LWD requires no architectural modifications and adds zero additional cost during inference, making it a practical solution for scaling existing models. Across multiple strong baselines, LWD consistently improves perceptual quality and FID scores, demonstrating the power of signal-driven supervision as a principled and efficient path toward high-resolution generative modeling.
<div id='section'>Paperid: <span id='pid'>35, <a href='https://arxiv.org/pdf/2503.09830.pdf' target='_blank'>https://arxiv.org/pdf/2503.09830.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Feng Zhou, Pu Cao, Yiyang Ma, Lu Yang, Jianqin Yin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.09830">Exploring Position Encoding in Diffusion U-Net for Training-free High-resolution Image Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Denoising higher-resolution latents via a pre-trained U-Net leads to repetitive and disordered image patterns. Although recent studies make efforts to improve generative quality by aligning denoising process across original and higher resolutions, the root cause of suboptimal generation is still lacking exploration. Through comprehensive analysis of position encoding in U-Net, we attribute it to inconsistent position encoding, sourced by the inadequate propagation of position information from zero-padding to latent features in convolution layers as resolution increases. To address this issue, we propose a novel training-free approach, introducing a Progressive Boundary Complement (PBC) method. This method creates dynamic virtual image boundaries inside the feature map to enhance position information propagation, enabling high-quality and rich-content high-resolution image synthesis. Extensive experiments demonstrate the superiority of our method.
<div id='section'>Paperid: <span id='pid'>36, <a href='https://arxiv.org/pdf/2401.15105.pdf' target='_blank'>https://arxiv.org/pdf/2401.15105.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jialu Sui, Yiyang Ma, Wenhan Yang, Xiaokang Zhang, Man-On Pun, Jiaying Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.15105">Diffusion Enhancement for Cloud Removal in Ultra-Resolution Remote Sensing Imagery</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The presence of cloud layers severely compromises the quality and effectiveness of optical remote sensing (RS) images. However, existing deep-learning (DL)-based Cloud Removal (CR) techniques encounter difficulties in accurately reconstructing the original visual authenticity and detailed semantic content of the images. To tackle this challenge, this work proposes to encompass enhancements at the data and methodology fronts. On the data side, an ultra-resolution benchmark named CUHK Cloud Removal (CUHK-CR) of 0.5m spatial resolution is established. This benchmark incorporates rich detailed textures and diverse cloud coverage, serving as a robust foundation for designing and assessing CR models. From the methodology perspective, a novel diffusion-based framework for CR called Diffusion Enhancement (DE) is proposed to perform progressive texture detail recovery, which mitigates the training difficulty with improved inference accuracy. Additionally, a Weight Allocation (WA) network is developed to dynamically adjust the weights for feature fusion, thereby further improving performance, particularly in the context of ultra-resolution image generation. Furthermore, a coarse-to-fine training strategy is applied to effectively expedite training convergence while reducing the computational complexity required to handle ultra-resolution images. Extensive experiments on the newly established CUHK-CR and existing datasets such as RICE confirm that the proposed DE framework outperforms existing DL-based methods in terms of both perceptual quality and signal fidelity.
<div id='section'>Paperid: <span id='pid'>37, <a href='https://arxiv.org/pdf/2507.21690.pdf' target='_blank'>https://arxiv.org/pdf/2507.21690.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sangmin Han, Jinho Jeong, Jinwoo Kim, Seon Joo Kim
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.21690">APT: Improving Diffusion Models for High Resolution Image Generation with Adaptive Path Tracing</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Latent Diffusion Models (LDMs) are generally trained at fixed resolutions, limiting their capability when scaling up to high-resolution images. While training-based approaches address this limitation by training on high-resolution datasets, they require large amounts of data and considerable computational resources, making them less practical. Consequently, training-free methods, particularly patch-based approaches, have become a popular alternative. These methods divide an image into patches and fuse the denoising paths of each patch, showing strong performance on high-resolution generation. However, we observe two critical issues for patch-based approaches, which we call ``patch-level distribution shift" and ``increased patch monotonicity." To address these issues, we propose Adaptive Path Tracing (APT), a framework that combines Statistical Matching to ensure patch distributions remain consistent in upsampled latents and Scale-aware Scheduling to deal with the patch monotonicity. As a result, APT produces clearer and more refined details in high-resolution images. In addition, APT enables a shortcut denoising process, resulting in faster sampling with minimal quality degradation. Our experimental results confirm that APT produces more detailed outputs with improved inference speed, providing a practical approach to high-resolution image generation.
<div id='section'>Paperid: <span id='pid'>38, <a href='https://arxiv.org/pdf/2311.16973.pdf' target='_blank'>https://arxiv.org/pdf/2311.16973.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ruoyi Du, Dongliang Chang, Timothy Hospedales, Yi-Zhe Song, Zhanyu Ma
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2311.16973">DemoFusion: Democratising High-Resolution Image Generation With No $$$</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>High-resolution image generation with Generative Artificial Intelligence (GenAI) has immense potential but, due to the enormous capital investment required for training, it is increasingly centralised to a few large corporations, and hidden behind paywalls. This paper aims to democratise high-resolution GenAI by advancing the frontier of high-resolution generation while remaining accessible to a broad audience. We demonstrate that existing Latent Diffusion Models (LDMs) possess untapped potential for higher-resolution image generation. Our novel DemoFusion framework seamlessly extends open-source GenAI models, employing Progressive Upscaling, Skip Residual, and Dilated Sampling mechanisms to achieve higher-resolution image generation. The progressive nature of DemoFusion requires more passes, but the intermediate results can serve as "previews", facilitating rapid prompt iteration.
<div id='section'>Paperid: <span id='pid'>39, <a href='https://arxiv.org/pdf/2506.20452.pdf' target='_blank'>https://arxiv.org/pdf/2506.20452.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tobias Vontobel, Seyedmorteza Sadat, Farnood Salehi, Romann M. Weber
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.20452">HiWave: Training-Free High-Resolution Image Generation via Wavelet-Based Diffusion Sampling</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Diffusion models have emerged as the leading approach for image synthesis, demonstrating exceptional photorealism and diversity. However, training diffusion models at high resolutions remains computationally prohibitive, and existing zero-shot generation techniques for synthesizing images beyond training resolutions often produce artifacts, including object duplication and spatial incoherence. In this paper, we introduce HiWave, a training-free, zero-shot approach that substantially enhances visual fidelity and structural coherence in ultra-high-resolution image synthesis using pretrained diffusion models. Our method employs a two-stage pipeline: generating a base image from the pretrained model followed by a patch-wise DDIM inversion step and a novel wavelet-based detail enhancer module. Specifically, we first utilize inversion methods to derive initial noise vectors that preserve global coherence from the base image. Subsequently, during sampling, our wavelet-domain detail enhancer retains low-frequency components from the base image to ensure structural consistency, while selectively guiding high-frequency components to enrich fine details and textures. Extensive evaluations using Stable Diffusion XL demonstrate that HiWave effectively mitigates common visual artifacts seen in prior methods, achieving superior perceptual quality. A user study confirmed HiWave's performance, where it was preferred over the state-of-the-art alternative in more than 80% of comparisons, highlighting its effectiveness for high-quality, ultra-high-resolution image synthesis without requiring retraining or architectural modifications.
<div id='section'>Paperid: <span id='pid'>40, <a href='https://arxiv.org/pdf/2503.18352.pdf' target='_blank'>https://arxiv.org/pdf/2503.18352.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jinjin Zhang, Qiuyu Huang, Junjie Liu, Xiefan Guo, Di Huang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.18352">Diffusion-4K: Ultra-High-Resolution Image Synthesis with Latent Diffusion Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper, we present Diffusion-4K, a novel framework for direct ultra-high-resolution image synthesis using text-to-image diffusion models. The core advancements include: (1) Aesthetic-4K Benchmark: addressing the absence of a publicly available 4K image synthesis dataset, we construct Aesthetic-4K, a comprehensive benchmark for ultra-high-resolution image generation. We curated a high-quality 4K dataset with carefully selected images and captions generated by GPT-4o. Additionally, we introduce GLCM Score and Compression Ratio metrics to evaluate fine details, combined with holistic measures such as FID, Aesthetics and CLIPScore for a comprehensive assessment of ultra-high-resolution images. (2) Wavelet-based Fine-tuning: we propose a wavelet-based fine-tuning approach for direct training with photorealistic 4K images, applicable to various latent diffusion models, demonstrating its effectiveness in synthesizing highly detailed 4K images. Consequently, Diffusion-4K achieves impressive performance in high-quality image synthesis and text prompt adherence, especially when powered by modern large-scale diffusion models (e.g., SD3-2B and Flux-12B). Extensive experimental results from our benchmark demonstrate the superiority of Diffusion-4K in ultra-high-resolution image synthesis.
<div id='section'>Paperid: <span id='pid'>41, <a href='https://arxiv.org/pdf/2405.14477.pdf' target='_blank'>https://arxiv.org/pdf/2405.14477.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Seyedmorteza Sadat, Jakob Buhmann, Derek Bradley, Otmar Hilliges, Romann M. Weber
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.14477">LiteVAE: Lightweight and Efficient Variational Autoencoders for Latent Diffusion Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Advances in latent diffusion models (LDMs) have revolutionized high-resolution image generation, but the design space of the autoencoder that is central to these systems remains underexplored. In this paper, we introduce LiteVAE, a new autoencoder design for LDMs, which leverages the 2D discrete wavelet transform to enhance scalability and computational efficiency over standard variational autoencoders (VAEs) with no sacrifice in output quality. We investigate the training methodologies and the decoder architecture of LiteVAE and propose several enhancements that improve the training dynamics and reconstruction quality. Our base LiteVAE model matches the quality of the established VAEs in current LDMs with a six-fold reduction in encoder parameters, leading to faster training and lower GPU memory requirements, while our larger model outperforms VAEs of comparable complexity across all evaluated metrics (rFID, LPIPS, PSNR, and SSIM).
<div id='section'>Paperid: <span id='pid'>42, <a href='https://arxiv.org/pdf/2507.05148.pdf' target='_blank'>https://arxiv.org/pdf/2507.05148.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chun Xie, Yuichi Yoshii, Itaru Kitahara
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.05148">SV-DRR: High-Fidelity Novel View X-Ray Synthesis Using Diffusion Model</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>X-ray imaging is a rapid and cost-effective tool for visualizing internal human anatomy. While multi-view X-ray imaging provides complementary information that enhances diagnosis, intervention, and education, acquiring images from multiple angles increases radiation exposure and complicates clinical workflows. To address these challenges, we propose a novel view-conditioned diffusion model for synthesizing multi-view X-ray images from a single view. Unlike prior methods, which are limited in angular range, resolution, and image quality, our approach leverages the Diffusion Transformer to preserve fine details and employs a weak-to-strong training strategy for stable high-resolution image generation. Experimental results demonstrate that our method generates higher-resolution outputs with improved control over viewing angles. This capability has significant implications not only for clinical applications but also for medical education and data extension, enabling the creation of diverse, high-quality datasets for training and analysis. Our code is available at GitHub.
<div id='section'>Paperid: <span id='pid'>43, <a href='https://arxiv.org/pdf/2212.11614.pdf' target='_blank'>https://arxiv.org/pdf/2212.11614.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shu Lok Tsang, Maxwell T. West, Sarah M. Erfani, Muhammad Usman
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2212.11614">Hybrid Quantum-Classical Generative Adversarial Network for High Resolution Image Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Quantum machine learning (QML) has received increasing attention due to its potential to outperform classical machine learning methods in problems pertaining classification and identification tasks. A subclass of QML methods is quantum generative adversarial networks (QGANs) which have been studied as a quantum counterpart of classical GANs widely used in image manipulation and generation tasks. The existing work on QGANs is still limited to small-scale proof-of-concept examples based on images with significant downscaling. Here we integrate classical and quantum techniques to propose a new hybrid quantum-classical GAN framework. We demonstrate its superior learning capabilities by generating $28 \times 28$ pixels grey-scale images without dimensionality reduction or classical pre/post-processing on multiple classes of the standard MNIST and Fashion MNIST datasets, which achieves comparable results to classical frameworks with three orders of magnitude less trainable generator parameters. To gain further insight into the working of our hybrid approach, we systematically explore the impact of its parameter space by varying the number of qubits, the size of image patches, the number of layers in the generator, the shape of the patches and the choice of prior distribution. Our results show that increasing the quantum generator size generally improves the learning capability of the network. The developed framework provides a foundation for future design of QGANs with optimal parameter set tailored for complex image generation tasks.
<div id='section'>Paperid: <span id='pid'>44, <a href='https://arxiv.org/pdf/2504.13622.pdf' target='_blank'>https://arxiv.org/pdf/2504.13622.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Dawid KopeÄ, Wojciech KozÅowski, Maciej Wizerkaniuk, Dawid Krutul, Jan KocoÅ, Maciej ZiÄba
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.13622">SupResDiffGAN a new approach for the Super-Resolution task</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this work, we present SupResDiffGAN, a novel hybrid architecture that combines the strengths of Generative Adversarial Networks (GANs) and diffusion models for super-resolution tasks. By leveraging latent space representations and reducing the number of diffusion steps, SupResDiffGAN achieves significantly faster inference times than other diffusion-based super-resolution models while maintaining competitive perceptual quality. To prevent discriminator overfitting, we propose adaptive noise corruption, ensuring a stable balance between the generator and the discriminator during training. Extensive experiments on benchmark datasets show that our approach outperforms traditional diffusion models such as SR3 and I$^2$SB in efficiency and image quality. This work bridges the performance gap between diffusion- and GAN-based methods, laying the foundation for real-time applications of diffusion models in high-resolution image generation.
<div id='section'>Paperid: <span id='pid'>45, <a href='https://arxiv.org/pdf/2411.03999.pdf' target='_blank'>https://arxiv.org/pdf/2411.03999.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ziji Shi, Jialin Li, Yang You
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.03999">ParaGAN: A Scalable Distributed Training Framework for Generative Adversarial Networks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advances in Generative Artificial Intelligence have fueled numerous applications, particularly those involving Generative Adversarial Networks (GANs), which are essential for synthesizing realistic photos and videos. However, efficiently training GANs remains a critical challenge due to their computationally intensive and numerically unstable nature. Existing methods often require days or even weeks for training, posing significant resource and time constraints.
  In this work, we introduce ParaGAN, a scalable distributed GAN training framework that leverages asynchronous training and an asymmetric optimization policy to accelerate GAN training. ParaGAN employs a congestion-aware data pipeline and hardware-aware layout transformation to enhance accelerator utilization, resulting in over 30% improvements in throughput. With ParaGAN, we reduce the training time of BigGAN from 15 days to 14 hours while achieving 91% scaling efficiency. Additionally, ParaGAN enables unprecedented high-resolution image generation using BigGAN.
<div id='section'>Paperid: <span id='pid'>46, <a href='https://arxiv.org/pdf/2406.16476.pdf' target='_blank'>https://arxiv.org/pdf/2406.16476.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shuwei Shi, Wenbo Li, Yuechen Zhang, Jingwen He, Biao Gong, Yinqiang Zheng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.16476">ResMaster: Mastering High-Resolution Image Generation via Structural and Fine-Grained Guidance</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Diffusion models excel at producing high-quality images; however, scaling to higher resolutions, such as 4K, often results in over-smoothed content, structural distortions, and repetitive patterns. To this end, we introduce ResMaster, a novel, training-free method that empowers resolution-limited diffusion models to generate high-quality images beyond resolution restrictions. Specifically, ResMaster leverages a low-resolution reference image created by a pre-trained diffusion model to provide structural and fine-grained guidance for crafting high-resolution images on a patch-by-patch basis. To ensure a coherent global structure, ResMaster meticulously aligns the low-frequency components of high-resolution patches with the low-resolution reference at each denoising step. For fine-grained guidance, tailored image prompts based on the low-resolution reference and enriched textual prompts produced by a vision-language model are incorporated. This approach could significantly mitigate local pattern distortions and improve detail refinement. Extensive experiments validate that ResMaster sets a new benchmark for high-resolution image generation and demonstrates promising efficiency. The project page is https://shuweis.github.io/ResMaster .
<div id='section'>Paperid: <span id='pid'>47, <a href='https://arxiv.org/pdf/2404.19475.pdf' target='_blank'>https://arxiv.org/pdf/2404.19475.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Teng Zhou, Yongchuan Tang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.19475">TwinDiffusion: Enhancing Coherence and Efficiency in Panoramic Image Generation with Diffusion Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Diffusion models have emerged as effective tools for generating diverse and high-quality content. However, their capability in high-resolution image generation, particularly for panoramic images, still faces challenges such as visible seams and incoherent transitions. In this paper, we propose TwinDiffusion, an optimized framework designed to address these challenges through two key innovations: the Crop Fusion for quality enhancement and the Cross Sampling for efficiency optimization. We introduce a training-free optimizing stage to refine the similarity of adjacent image areas, as well as an interleaving sampling strategy to yield dynamic patches during the cropping process. A comprehensive evaluation is conducted to compare TwinDiffusion with the prior works, considering factors including coherence, fidelity, compatibility, and efficiency. The results demonstrate the superior performance of our approach in generating seamless and coherent panoramas, setting a new standard in quality and efficiency for panoramic image generation.
<div id='section'>Paperid: <span id='pid'>48, <a href='https://arxiv.org/pdf/2404.07754.pdf' target='_blank'>https://arxiv.org/pdf/2404.07754.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tuong Vy Nguyen, Alexander Glaser, Felix Biessmann
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.07754">Generating Synthetic Satellite Imagery With Deep-Learning Text-to-Image Models -- Technical Challenges and Implications for Monitoring and Verification</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Novel deep-learning (DL) architectures have reached a level where they can generate digital media, including photorealistic images, that are difficult to distinguish from real data. These technologies have already been used to generate training data for Machine Learning (ML) models, and large text-to-image models like DALL-E 2, Imagen, and Stable Diffusion are achieving remarkable results in realistic high-resolution image generation. Given these developments, issues of data authentication in monitoring and verification deserve a careful and systematic analysis: How realistic are synthetic images? How easily can they be generated? How useful are they for ML researchers, and what is their potential for Open Science? In this work, we use novel DL models to explore how synthetic satellite images can be created using conditioning mechanisms. We investigate the challenges of synthetic satellite image generation and evaluate the results based on authenticity and state-of-the-art metrics. Furthermore, we investigate how synthetic data can alleviate the lack of data in the context of ML methods for remote-sensing. Finally we discuss implications of synthetic satellite imagery in the context of monitoring and verification.
<div id='section'>Paperid: <span id='pid'>49, <a href='https://arxiv.org/pdf/2504.13214.pdf' target='_blank'>https://arxiv.org/pdf/2504.13214.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Andrew Kiruluta
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.13214">Wavelet-based Variational Autoencoders for High-Resolution Image Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Variational Autoencoders (VAEs) are powerful generative models capable of learning compact latent representations. However, conventional VAEs often generate relatively blurry images due to their assumption of an isotropic Gaussian latent space and constraints in capturing high-frequency details. In this paper, we explore a novel wavelet-based approach (Wavelet-VAE) in which the latent space is constructed using multi-scale Haar wavelet coefficients. We propose a comprehensive method to encode the image features into multi-scale detail and approximation coefficients and introduce a learnable noise parameter to maintain stochasticity. We thoroughly discuss how to reformulate the reparameterization trick, address the KL divergence term, and integrate wavelet sparsity principles into the training objective. Our experimental evaluation on CIFAR-10 and other high-resolution datasets demonstrates that the Wavelet-VAE improves visual fidelity and recovers higher-resolution details compared to conventional VAEs. We conclude with a discussion of advantages, potential limitations, and future research directions for wavelet-based generative modeling.
<div id='section'>Paperid: <span id='pid'>50, <a href='https://arxiv.org/pdf/2411.10180.pdf' target='_blank'>https://arxiv.org/pdf/2411.10180.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Siddharth Roheda
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.10180">CART: Compositional Auto-Regressive Transformer for Image Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In recent years, image synthesis has achieved remarkable advancements, enabling diverse applications in content creation, virtual reality, and beyond. We introduce a novel approach to image generation using Auto-Regressive (AR) modeling, which leverages a next-detail prediction strategy for enhanced fidelity and scalability. While AR models have achieved transformative success in language modeling, replicating this success in vision tasks has presented unique challenges due to the inherent spatial dependencies in images. Our proposed method addresses these challenges by iteratively adding finer details to an image compositionally, constructing it as a hierarchical combination of base and detail image factors. This strategy is shown to be more effective than the conventional next-token prediction and even surpasses the state-of-the-art next-scale prediction approaches. A key advantage of this method is its scalability to higher resolutions without requiring full model retraining, making it a versatile solution for high-resolution image generation.
<div id='section'>Paperid: <span id='pid'>51, <a href='https://arxiv.org/pdf/2302.02412.pdf' target='_blank'>https://arxiv.org/pdf/2302.02412.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ãlvaro Barbero JimÃ©nez
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2302.02412">Mixture of Diffusers for scene composition and high resolution image generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Diffusion methods have been proven to be very effective to generate images while conditioning on a text prompt. However, and although the quality of the generated images is unprecedented, these methods seem to struggle when trying to generate specific image compositions. In this paper we present Mixture of Diffusers, an algorithm that builds over existing diffusion models to provide a more detailed control over composition. By harmonizing several diffusion processes acting on different regions of a canvas, it allows generating larger images, where the location of each object and style is controlled by a separate diffusion process.
<div id='section'>Paperid: <span id='pid'>52, <a href='https://arxiv.org/pdf/2008.09646.pdf' target='_blank'>https://arxiv.org/pdf/2008.09646.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Abhinav Sagar
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2008.09646">HRVGAN: High Resolution Video Generation using Spatio-Temporal GAN</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>High-resolution video generation has emerged as a crucial task in computer vision, with wide-ranging applications in entertainment, simulation, and data augmentation. However, generating temporally coherent and visually realistic videos remains a significant challenge due to the high dimensionality and complex dynamics of video data. In this paper, we propose a novel deep generative network architecture designed specifically for high-resolution video synthesis. Our approach integrates key concepts from Wasserstein Generative Adversarial Networks (WGANs), enforcing a k-Lipschitz continuity constraint on the discriminator to stabilize training and enhance convergence. We further leverage Conditional GAN (cGAN) techniques by incorporating class labels during both training and inference, enabling class-specific video generation with improved semantic consistency. We provide a detailed layer-wise description of the Generator and Discriminator networks, highlighting architectural design choices promoting temporal coherence and spatial detail. The overall combined architecture, training algorithm, and optimization strategy are thoroughly presented. Our training objective combines a pixel-wise mean squared error loss with an adversarial loss to balance frame-level accuracy and video realism. We validate our approach on benchmark datasets including UCF101, Golf, and Aeroplane, encompassing diverse motion patterns and scene contexts. Quantitative evaluations using Inception Score (IS) and FrÃ©chet Inception Distance (FID) demonstrate that our model significantly outperforms previous state-of-the-art unsupervised video generation methods in terms of both quality and diversity.
